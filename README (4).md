# ğŸ›¡ï¸ Advanced Fraud Detection for E-commerce and Banking

## ğŸ“‹ Project Overview
This repository contains a comprehensive machine learning pipeline for Adey Innovations Inc., designed to enhance the detection of fraudulent transactions in both e-commerce and banking sectors. The project tackles the critical business need to minimize financial losses from fraud while ensuring a seamless and positive user experience by reducing false positives.

By leveraging advanced data analysis, feature engineering, and machine learning, this project delivers a robust solution that can identify complex fraud patterns from raw transaction data.


## ğŸ¯ Key Features

- âœ… **Modular Data Pipeline** - Reusable preprocessing pipeline for cleaning and preparing transaction data
- âœ… **In-depth EDA** - Comprehensive exploratory data analysis to uncover hidden fraud patterns
- âœ… **Strategic Feature Engineering** - Create powerful, predictive signals from temporal and user-behavior data
- âœ… **Class Imbalance Handling** - Clear plan for handling severe class imbalance in fraud detection
- âœ… **Well-documented Structure** - Support for model building, evaluation, and interpretation

> ğŸ’¡ This project serves as a foundational step in building an intelligent system that not only detects fraud but also provides insights into the drivers of fraudulent behavior.

---

## ğŸ“ Project Structure
fraud_detection_project/
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚ Â  â”œâ”€â”€ ğŸ“‚ raw/
â”‚ Â  â”‚ Â  â”œâ”€â”€ ğŸ“„ Fraud_Data.csv
â”‚ Â  â”‚ Â  â”œâ”€â”€ ğŸ“„ IpAddress_to_Country.csv
â”‚ Â  â”‚ Â  â””â”€â”€ ğŸ“„ creditcard.csv
â”‚ Â  â””â”€â”€ ğŸ“‚ processed/
â”‚ Â  Â  Â  â””â”€â”€ ğŸ“„ processed_fraud_data.csv
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚ Â  â”œâ”€â”€ ğŸ““ 1_Data_Processing_and_EDA.ipynb
â”‚ Â  â””â”€â”€ ğŸ““ 2_Model_Training_and_Evaluation.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚ Â  â”œâ”€â”€ ğŸ __init__.py
â”‚ Â  â”œâ”€â”€ âš™ï¸ config.py
â”‚ Â  â”œâ”€â”€ ğŸ”§ task1_pipeline.py
â”‚ Â  â””â”€â”€ ğŸ§  task2_modeling_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/
â”‚   â””â”€â”€ ï¿½ï¸ images/
â”‚
â””â”€â”€ ğŸ“– README.md

## ğŸš€ Setup Instructions

### 1. ğŸ“¥ Clone the Repository

```bash
git clone https://github.com/matos-coder/Improved-detection-of-fraud-cases-for-e-commerce-and-bank-transactions
cd 'Improved-detection-of-fraud-cases-for-e-commerce-and-bank-transactions'
```

### 2. ğŸ Create a Virtual Environment

It is **highly recommended** to use a virtual environment to manage project dependencies.

#### Windows
```powershell
python -m venv venv
venv\Scripts\activate
```

#### macOS/Linux
```bash
python3 -m venv venv
source venv/bin/activate
```

pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn lightgbm

Or install from a requirements.txt file:

pip install -r requirements.txt

## ğŸ¯ Task 1: Data Analysis and Preprocessing
Objective: To thoroughly clean, analyze, and enrich the e-commerce transaction dataset (Fraud_Data.csv) to prepare it for machine learning model training.

ğŸ“ Location: The entire workflow is orchestrated in notebooks/1_Data_Processing_and_EDA.ipynb, which calls modular functions from scripts/task1_pipeline.py.

### ğŸ§¹ Data Cleaning and Preprocessing
Goal: To ensure data quality and consistency. This is a critical first step as model performance is highly dependent on clean data.


#### Implementation:

| Step | Description | Justification |
|------|-------------|---------------|
| ğŸ”„ **Handled Duplicates** | Removed identical rows | Prevents data leakage and model bias |
| ğŸ“… **Corrected Data Types** | Converted `signup_time` and `purchase_time` to datetime | Essential for time-based calculations |
| ğŸŒ **Handled IP Addresses** | Converted `ip_address` from float to integer | Facilitates merging with geolocation dataset |

### ğŸ“Š Exploratory Data Analysis (EDA)

**Goal:** To understand the underlying patterns in the data, especially the differences between fraudulent and legitimate transactions.

#### ğŸ” Analysis Components:

- **ğŸ“ˆ Class Imbalance Analysis** - Visualized the severe imbalance between fraud and non-fraud classes
- **ğŸ“Š Univariate Analysis** - Analyzed distributions of key numerical features
- **ğŸ“ˆ Bivariate Analysis** - Created boxplots comparing feature distributions across classes

#### ğŸ’¡ Key Insights from EDA:

| Insight | Description | Impact |
|---------|-------------|--------|
| âš ï¸ **Severe Class Imbalance** | Fraudulent transactions: ~9% of dataset | Accuracy is poor metric; requires SMOTE/AUC-PR |
| â° **Time-Based Patterns** | Fraud occurs faster after signup | Strong indicator of fraudulent behavior |
| ğŸŒ **Geographic Hotspots** | Fraud not evenly distributed globally | Certain countries show higher fraud rates |

### ğŸ”§ Feature Engineering

**Goal:** To create new, high-signal features that explicitly capture behaviors associated with fraud.

#### ğŸ› ï¸ Engineered Features:

| Feature | Description | Fraud Signal |
|---------|-------------|--------------|
| â±ï¸ `time_since_signup_seconds` | Duration between signup and first purchase | Short duration = fraud risk |
| ğŸ• `purchase_hour_of_day` | Hour when purchase was made | Overnight purchases = higher risk |
| ğŸ“… `purchase_day_of_week` | Day of week for purchase | Pattern detection |
| ğŸ“± `device_id_count` | Transactions per device | High frequency = automation |
| ğŸ‘¤ `user_id_count` | Transactions per user | Behavioral analysis |
| ğŸŒ `country` | Geographic location from IP | Regional fraud patterns |

> ğŸ’¡ **Justification:** These engineered features are designed based on well-known fraud typologies, such as fraudsters creating accounts and using them immediately.

## ğŸ¯ Task 2: Model Building and Training
Objective: To build, train, and evaluate machine learning models to accurately detect fraudulent transactions on both the e-commerce and credit card datasets.

ğŸ“ Location: The workflow is orchestrated in notebooks/2_Model_Training_and_Evaluation.ipynb, with logic encapsulated in scripts/task2_modeling_pipeline.py.

### ğŸ§  Model Selection
Goal: To compare a simple, interpretable baseline model against a powerful, complex ensemble model to find the best solution for the business problem.


| Model | Type | Justification |
|---|---|---|
| ğŸ“ˆ Logistic Regression | Baseline | A simple, fast, and highly interpretable model. Excellent for establishing a performance benchmark. |
| ğŸŒ³ LightGBM | Powerful Ensemble | A gradient boosting framework known for high performance and efficiency. It can capture complex, non-linear patterns that simpler models miss. |

### âš™ï¸ Data Preparation and Preprocessing
Goal: To prepare the data for the machine learning models by scaling numerical features and encoding categorical ones.


ğŸ“‹ Strategy:

| Component | Approach | Reasoning |
|---|---|---|
| âœ‚ï¸ Train-Test Split | 80/20 split using stratify | Ensures the same percentage of fraud cases in both training and testing sets, which is crucial for imbalanced data. |
| âš–ï¸ Numerical Scaling | StandardScaler | Scales numerical features to have a mean of 0 and standard deviation of 1, preventing features with large ranges from dominating the model. |
| ğŸ·ï¸ Categorical Encoding | OneHotEncoder | Converts categorical text data (e.g., 'Chrome', 'Ads') into a numerical format that the models can understand. |

### âš–ï¸ Handling Class Imbalance (SMOTE)
Goal: To address the severe class imbalance identified in Task 1 to ensure the model learns to identify the minority (fraud) class effectively.


| Component | Approach | Reasoning |
|---|---|---|
| ğŸ¯ Chosen Technique | SMOTE (Synthetic Minority Over-sampling) | Creates new, synthetic examples of the minority class in the training data only. |
| ğŸ’¡ Justification |  | By balancing the class distribution, SMOTE forces the model to learn the patterns of fraudulent transactions instead of just ignoring them. This is critical for building a useful fraud detection system. |

## ğŸ“ˆ Model Training and Evaluation
Goal: To train the selected models and rigorously evaluate their performance using metrics appropriate for imbalanced classification.


ğŸ“Š Evaluation Metrics:

| Metric | What It Measures | Why It's Important for Fraud |
|---|---|---|
| ğŸ¯ F1-Score | The harmonic mean of Precision and Recall. | Provides a single score that balances the cost of false positives (annoying customers) and false negatives (missing fraud). |
| ğŸ“ˆ AUC-PR | Area Under the Precision-Recall Curve. | The best metric for imbalanced data. It evaluates the model's ability to distinguish between classes across all possible decision thresholds. A higher score means a more robust model. |
| ğŸ”¢ Confusion Matrix | A table showing True Positives, True Negatives, False Positives, and False Negatives. | Gives a detailed breakdown of the model's prediction accuracy for both classes. |

## ğŸ† Results and Justification
The performance of both models was evaluated on both the e-commerce and credit card datasets.


ğŸ“Š Final Performance Summary:

| Dataset | Model | F1 Score | AUC-PR |
|---|---|---|---|
| E-commerce | Logistic Regression | 0.6133 | 0.7634 |
| E-commerce | LightGBM | 0.8407 | 0.8872 |
| Credit Card | Logistic Regression | 0.1171 | 0.7637 |
| Credit Card | LightGBM | 0.8614 | 0.8491 |

ğŸ’¡ Conclusion: LightGBM is the Best Model
Justification:
Across both datasets, the LightGBM model demonstrates overwhelmingly superior performance.

Higher F1-Score and AUC-PR: The significantly higher scores indicate that LightGBM is far better at correctly identifying fraudulent transactions while maintaining a low false positive rate.

Business Value: For Adey Innovations Inc., this translates directly to more fraud caught and fewer legitimate customers impacted, maximizing revenue protection and customer satisfaction.

While Logistic Regression provides a useful baseline, its performance is insufficient for a production-level fraud detection system. The advanced capabilities of LightGBM are essential for this business use case.

## ğŸš€ How to Run the Pipeline
ğŸ““ Task 1 - Data Processing and EDA
Execute the complete pipeline by running the Jupyter Notebook:


```bash
# Navigate to the notebook and run the cells
jupyter notebook notebooks/1_Data_Processing_and_EDA.ipynb
```

ğŸ§  Task 2 - Model Building and Training
Execute the modeling pipeline by running the second Jupyter Notebook:

```bash
# Navigate to the notebook and run the cells
jupyter notebook notebooks/2_Model_Training_and_Evaluation.ipynb
```

This will:

- ğŸ“¥ Load the processed e-commerce data and raw credit card data.
- âš™ï¸ Preprocess, split, and balance the data.
- ğŸ§  Train and evaluate both Logistic Regression and LightGBM models.
- ğŸ“Š Display performance metrics and confusion matrices for each model.
- ğŸ† Provide a final summary and justification for the best model.

## ğŸ¤ Contributing

ğŸ´ Fork the repository
ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
ğŸ’¾ Commit your changes (`git commit -m 'Add some amazing feature'`)
ğŸ“¤ Push to the branch (`git push origin feature/amazing-feature`)
ğŸ”„ Open a Pull Request

## ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Contact
Adey Innovations Inc.

ğŸ“§ Email: contact@adeyinnovations.com

ğŸŒ Website: www.adeyinnovations.com

ğŸ’¼ LinkedIn: Adey Innovations

